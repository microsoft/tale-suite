{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook for pulling the run data from wandb for TALES and creating the various figures and tables used in our paper and website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Relevant Variables:\n",
    "- tmp: dataframe with all of the run info\n",
    "- framework_games: a dictionary of the form {framework: [all games in framework]}. Useful for checking and making sure that every llm has at least one game run\n",
    "- llm_framework_scores: a dictionary of the form {llm: {framework: {game: {seed: [best_score, tokens_used]}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These cells do the following:\n",
    "- Initialize the wandb api\n",
    "- Pull all the runs according to certain filters ('conversation: True')\n",
    "- Dump the runs into a pandas dataframe\n",
    "- Print the number of total games per llm (This should be 615 if all games were run 615 = 123 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workspace you want to pull runs from\n",
    "wandb_workspace = \"text-games-benchmark\"\n",
    "\n",
    "# Where you want the logged format texts to be stored (option for more complicated file structures later down)\n",
    "base_path = '/root/logs/'\n",
    "\n",
    "# Divider between each step. Change as wanted\n",
    "divider_string = \"\\n************************************************************************************************\\n\"\n",
    "\n",
    "# The specific chain of thought prompt: probably not needed, will remove in the future\n",
    "COT_SYS_PROMPT = \"\"\"You are playing a text-based game and your goal is to finish it with the highest score. Upon reading the text observation, generate a plan with subgoals when asked to think step-by-step, then provide a *single* short phrase to interact with the game when asked to do so, e.g. `get lamp` (without the backticks). When stuck, try using the `help` command to see what commands are available.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filter ={\n",
    "            \"config.conversation\": True,\n",
    "        }\n",
    "\n",
    "runs = api.runs(path=\"text-games-benchmark\", filters = filter)\n",
    "run_cutoff = \"2024-02-0\"\n",
    "files_skipped = 0\n",
    "path = base_path # Change to your file path\n",
    "runs_data = []\n",
    "for run in runs:\n",
    "    if 'without-help' not in run.tags:\n",
    "        run_data = dict(run.summary)\n",
    "        run_data.update(run.config)\n",
    "        run_data['tags'] = run.tags\n",
    "        run_data['run_object'] = run\n",
    "\n",
    "        # Download the artifact to extract the rollout. This can take a while if you haven't downloaded them before so I would recommend commenting them out if not needed\n",
    "        file_path = run._summary['episode/rollout']['path']\n",
    "        full_path = path + file_path\n",
    "        try:\n",
    "            if not os.path.exists(full_path): # If the log has not been downloaded, download it\n",
    "                print(\"Downloading file\", full_path)\n",
    "                for file in run.files():\n",
    "                    if \"rollout\" in file.name:\n",
    "                        file.download(base_path, exist_ok=True)\n",
    "            with open(full_path) as json_file: log = json.load(json_file)\n",
    "            run_data['rollout'] = log['data']\n",
    "        except:\n",
    "            print(\"Could not get trajectory for run\", run.name)\n",
    "            files_skipped += 1\n",
    "            if files_skipped > 5:\n",
    "                break\n",
    "\n",
    "        run_data[\"agent\"] = run.config[\"agent\"]\n",
    "        run_data[\"game\"] = run.config[\"game\"]\n",
    "        runs_data.append(run_data)\n",
    "print(len(runs_data))\n",
    "runs_df = pd.DataFrame(runs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the runs\n",
    "def regen_and_filter_df():\n",
    "    tmp = runs_df\n",
    "    tmp = tmp[tmp['conversation'] == True]\n",
    "    tmp1 = tmp[tmp['agent_type'] == 'zero-shot']\n",
    "    tmp2 = tmp[tmp['agent_type'] == 'react']\n",
    "    # tmp = tmp[tmp['agent_type'] == 'zero-shot']\n",
    "    tmp = pd.concat([tmp1, tmp2])\n",
    "    tmp = tmp[tmp['llm'] != 'claude-3.5-sonnet']\n",
    "    tmp = tmp[tmp['llm'] != 'o3-mini']\n",
    "    tmp = tmp[tmp['game'] != 'JerichoEnvTheatre']\n",
    "    tmp = tmp[~tmp['tags'].apply(lambda tags: 'without-help' in tags)]\n",
    "    llm_counts = tmp[['llm', 'seed', 'game']].groupby('llm').size().reset_index(name='total_games')\n",
    "    print(llm_counts)\n",
    "    return tmp\n",
    "\n",
    "tmp = regen_and_filter_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a dictionary of the form {llm: {framework: {game: {seed: best_score}}}} to make things easier later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_runs = 0\n",
    "num_traj = 0\n",
    "avg_last_rewarded_step = 0\n",
    "rewarded_trajs = 0\n",
    "filtered_runs_data = tmp.to_dict(orient='records')\n",
    "data = random.choice(filtered_runs_data)\n",
    "print(data.keys())\n",
    "all_frameworks = tmp['framework'].unique()\n",
    "all_llms = tmp['llm'].unique()\n",
    "print(all_frameworks)\n",
    "# Could just do tmp['framework'].unique() but this enforces ordering\n",
    "fws = ['textworld', 'textworld_express', 'alfworld', 'scienceworld', 'jericho']\n",
    "# store everything in a dictionary of the form {llm: {framework: {game: {seed: best_score}}}}\n",
    "llm_framework_scores = {} \n",
    "\n",
    "# Iniitialize the LLM layer of the dictionary\n",
    "for llm in all_llms:\n",
    "    llm_framework_scores[llm] = {}\n",
    "\n",
    "# Initialize the framework layer of the dictionary\n",
    "for framework in fws:\n",
    "    for llm in all_llms:\n",
    "        llm_framework_scores[llm][framework] = {}   \n",
    "\n",
    "# For game and seeds, its easier to just iterate through the filtered_runs_data\n",
    "for data in filtered_runs_data:\n",
    "    game = data['game']\n",
    "    seed = data['seed']\n",
    "\n",
    "    if game not in llm_framework_scores[data['llm']][data['framework']].keys():\n",
    "        llm_framework_scores[data['llm']][data['framework']][game] = {}\n",
    "    \n",
    "\n",
    "    # Get the max score reached\n",
    "    max_score = -1000000\n",
    "    if 'rollout' not in data.keys():\n",
    "        llm_framework_scores[data['llm']][data['framework']][game][seed] = 0\n",
    "        continue\n",
    "    for step in data['rollout']:\n",
    "        if step[3] > max_score:\n",
    "            max_score = step[3]\n",
    "\n",
    "    tokens_used = data['episode/token_usage']\n",
    "    llm_framework_scores[data['llm']][data['framework']][game][seed] = [max_score, tokens_used]\n",
    "\n",
    "# Uncomment to count the number of runs for each LLM to sanity check\n",
    "# for llm in all_llms:\n",
    "#     total_runs = 0\n",
    "#     for framework in fws:\n",
    "#         relevant_games = llm_framework_scores[llm][framework]\n",
    "#         for game, seeds in relevant_games.items():\n",
    "#             total_runs += len(seeds)\n",
    "#     print(\"LLM: \", llm)\n",
    "#     print(\"Total runs: \", total_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_framework_scores['o1']['textworld_express']['TWXCookingWorld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictionary of all frameworks and games\n",
    "- Useful for when you want to check that every game has at least one run. Of the form {framework: [games]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of the form {framework: [games]} to make things easier\n",
    "framework_games = {}\n",
    "for framework in fws:\n",
    "    framework_games[framework] = []\n",
    "    for llm in all_llms:\n",
    "        relevant_games = llm_framework_scores[llm][framework]\n",
    "        for game, seeds in relevant_games.items():\n",
    "            if game not in framework_games[framework]:\n",
    "                framework_games[framework].append(game)\n",
    "\n",
    "print(framework_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check why a model isn't appearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_llm = 'gemini-2.5-pro-preview-03-25'\n",
    "one_llm = 'gemini-2.0-flash'\n",
    "def get_missing_games(one_llm):\n",
    "    tmp_one_llms = tmp[tmp['llm'] == one_llm]\n",
    "    one_llm_games = tmp_one_llms['game'].unique()\n",
    "\n",
    "    all_games = []\n",
    "    for _, games in framework_games.items():\n",
    "        all_games += games\n",
    "\n",
    "    missing_games = []\n",
    "    for game in all_games:\n",
    "        if game not in one_llm_games:\n",
    "            missing_games.append(game)\n",
    "    return missing_games\n",
    "\n",
    "print(get_missing_games('gemini-2.0-flash'))\n",
    "print(\"______________________________________\")\n",
    "print(get_missing_games('gemini-2.5-pro-preview-03-25'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making and formatting table and charts for overleaf and the website.\n",
    "- Alot of these I made it so that the output of the cell can just be copy-pasted into the appropriate table\n",
    "- For updating the website with scores, you should just be able to run everything and the appropriate files will be saved in their respective folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all of the scores and stds\n",
    "- Need to run this for all of the following tables/charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cast_to_float(s):\n",
    "    try:\n",
    "        return round(float(s), 3)\n",
    "    except ValueError:\n",
    "        return None  # Return None if the conversion fails\n",
    "    \n",
    "def get_all_scores_and_stds(llms, frameworks, require_all_frameworks = True, require_all_games = True, require_all_seeds = False, skip_llms = ['claude-3.5-sonnet'], skip_frameworks = [], verbose = False):\n",
    "    scores_per_framework = []\n",
    "    stds_per_framework = []\n",
    "    # Literate through all llms:\n",
    "    for llm in llms:\n",
    "        # Skip the LLMs that are not in the list\n",
    "        if llm in skip_llms: continue\n",
    "        # Put the llm in the first position in the list\n",
    "        scores = [llm]\n",
    "        stds = [llm]\n",
    "        all_framework_scores = []\n",
    "        all_framework_stds = []\n",
    "        all_frameworks = []\n",
    "\n",
    "        # If all seeds have not been completed, add a * to the name\n",
    "        all_seeds_done = True\n",
    "        for framework in frameworks:\n",
    "            # Grab the relevant games for the framework\n",
    "            relevant_games = llm_framework_scores[llm][framework]\n",
    "\n",
    "            all_tokens = []\n",
    "            all_scores = []\n",
    "            all_stds = []\n",
    "            all_games = []\n",
    "            for game, seeds in relevant_games.items():\n",
    "                # Get the scores for the game per seed:\n",
    "                token_seeds = []\n",
    "                score_seeds = []\n",
    "                all_games.append(game)\n",
    "                for score_tokens in seeds.values():\n",
    "                    score_seeds.append(score_tokens[0])\n",
    "                    token_seeds.append(score_tokens[1])\n",
    "                all_scores.append(np.mean(score_seeds))\n",
    "                all_stds.append(np.std(score_seeds))\n",
    "                all_tokens.append(np.mean(token_seeds))\n",
    "                all_framework_scores.append(all_scores[-1])\n",
    "                all_framework_stds.append(all_stds[-1])\n",
    "                if len(seeds) < 4: \n",
    "                    all_seeds_done = False\n",
    "\n",
    "            # if we are requiring all games, check if we have all the games\n",
    "            missing_games = []\n",
    "            if require_all_games:\n",
    "                for game in framework_games[framework]: \n",
    "                    if game not in all_games: missing_games.append(game)\n",
    "                if len(missing_games) != 0: \n",
    "                    if verbose: print(f'{llm} missing games {\",\".join(missing_games)} in {framework}')\n",
    "                    continue\n",
    "\n",
    "            # if we are requiring all seeds, check if we have all the seeds\n",
    "            if require_all_seeds:\n",
    "                for game, seeds in relevant_games.items(): \n",
    "                    if len(seeds) != 5: missing_games.append(game)\n",
    "                if len(missing_games) != 0:\n",
    "                    if verbose: print(f'{llm} missing seeds {\",\".join(missing_games)} in {framework}')\n",
    "                    continue\n",
    "\n",
    "            all_frameworks.append(framework)\n",
    "            scores.append(cast_to_float(np.mean(all_scores)))\n",
    "            stds.append(cast_to_float(np.mean(all_stds)))\n",
    "\n",
    "        # if we are requiring all frameworks, check if we have all the frameworks\n",
    "        if require_all_frameworks:\n",
    "            if len(scores) != len(frameworks) + 1: # +1 offset for the llm\n",
    "                missing_frameworks = []\n",
    "                for framework in frameworks: \n",
    "                    if framework not in all_frameworks: missing_frameworks.append(framework)\n",
    "                if len(missing_frameworks) != 0:\n",
    "                    if verbose: print(f'{llm} missing frameworks {\",\".join(missing_frameworks)}')\n",
    "                    continue\n",
    "\n",
    "        # Append the per-game mean of all the scores to the end\n",
    "        scores.append(cast_to_float(np.mean(all_framework_scores)))\n",
    "        if not all_seeds_done: scores[0] += \"*\"\n",
    "        scores_per_framework.append(scores) \n",
    "\n",
    "        # Append the per-game mean of all of the standard deviations to the end\n",
    "        stds.append(cast_to_float(np.mean(all_framework_stds)))\n",
    "        if not all_seeds_done: stds[0] += \"*\"\n",
    "        stds_per_framework.append(stds)\n",
    "    \n",
    "    return scores_per_framework, stds_per_framework\n",
    "\n",
    "scores_per_framework, stds_per_framework = get_all_scores_and_stds(\n",
    "                                            llms = all_llms, \n",
    "                                            frameworks = fws, \n",
    "                                            require_all_frameworks = True, \n",
    "                                            require_all_games = True, \n",
    "                                            require_all_seeds = False, \n",
    "                                            skip_llms = ['claude-3.5-sonnet'], \n",
    "                                            skip_frameworks = [], \n",
    "                                            verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_str(value, sig_digits=3):\n",
    "    if value == 0: return '0.' + '0' * sig_digits\n",
    "    \n",
    "    # Count sig figs after decimal\n",
    "    sig_figs = len(str(value).split(\".\")[-1])\n",
    "\n",
    "    # Count zeros to be added:\n",
    "    trailing_zeros = sig_digits - sig_figs\n",
    "    \n",
    "    # Pad zeros\n",
    "    string_value = str(value)\n",
    "    for i in range(trailing_zeros): string_value += \"0\"\n",
    "\n",
    "    return string_value\n",
    "\n",
    "def sort_by_last_element(arr):\n",
    "    return sorted(arr, key=lambda x: x[-1], reverse = True)\n",
    "\n",
    "sorted_array = sort_by_last_element(scores_per_framework)\n",
    "print(\"Model, Textworld, Textworld_express, Alfworld, Scienceworld, Jericho, Overall\")\n",
    "\n",
    "# Padding them to have the &s line up because it makes my ocd happy\n",
    "formatted_rows = []\n",
    "for model in sorted_array:\n",
    "    formattedRow = [model[0].split(\"/\")[-1]]\n",
    "    for score in model[1:]:\n",
    "        formattedRow.append(cast_to_str(cast_to_float(score * 100), 1))\n",
    "    formatted_rows.append(formattedRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out a latex-formatted table of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_len = max([len(row[0]) for row in formatted_rows])\n",
    "\n",
    "for row in formatted_rows:\n",
    "    scores = \" & \" +  \" & \".join(row[1:])\n",
    "    print(f\"{row[0]:<{max_model_len + 2}} {scores} \\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_games_per_framework(tmp, framework):\n",
    "    llm_games = {}\n",
    "\n",
    "    all_games = sorted(framework_games[framework])\n",
    "    print(len(all_games))\n",
    "\n",
    "    for llm in tmp['llm'].unique():\n",
    "        llm_name_formatted = llm.split(\"/\")[-1]\n",
    "        game_scores = []\n",
    "        for game in all_games:\n",
    "            # Basing this on Marc's code\n",
    "            llm_filtered_tmp = tmp[tmp['llm'] == llm]\n",
    "            framework_filtered_tmp = llm_filtered_tmp[llm_filtered_tmp['game'] == game]\n",
    "            avg_score = framework_filtered_tmp.groupby(\"seed\")['episode/normalized_highscore'].mean().reset_index()\n",
    "            avg_score.columns = [\"seeds\", \"avg_normalized_highscore_per_episode\"]\n",
    "            game_scores.append(avg_score['avg_normalized_highscore_per_episode'].mean())\n",
    "\n",
    "        llm_games[llm_name_formatted] = game_scores\n",
    "\n",
    "    return llm_games, all_games\n",
    "\n",
    "def generate_latex_table(table_string):\n",
    "    # Split the string into lines and extract the header and rows\n",
    "    lines = [line.strip() for line in table_string.strip().splitlines() if line.strip()]\n",
    "    headers = [h.strip() for h in lines[0].split(\"&\")]\n",
    "    rows = [line.rstrip(\"\\\\\").strip().split(\"&\") for line in lines[1:]]\n",
    "\n",
    "    # Clean up each row value\n",
    "    cleaned_rows = [[cell.strip() for cell in row] for row in rows]\n",
    "\n",
    "    # Start LaTeX table\n",
    "    latex = []\n",
    "    latex.append(\"\\\\begin{table}[ht]\")\n",
    "    latex.append(\"\\\\centering\")\n",
    "    latex.append(\"\\\\scriptsize\")\n",
    "    latex.append(\"\\\\setlength{\\\\tabcolsep}{3pt}\")\n",
    "    latex.append(\"\\\\renewcommand{\\\\arraystretch}{1.1}\")\n",
    "    latex.append(\"\\\\begin{tabular}{\\\\textwidth}{l\" + \"c\" * (len(headers) - 1) + \"}\")\n",
    "    latex.append(\"\\\\toprule\")\n",
    "\n",
    "    # Format header row\n",
    "    formatted_headers = [headers[0]] + [f\"\\\\rotatebox{{280}}{{{h}}}\" for h in headers[1:]]\n",
    "    latex.append(\" & \".join(formatted_headers) + \" \\\\\\\\\")\n",
    "    latex.append(\"\\\\midrule\")\n",
    "\n",
    "    # Format each data row\n",
    "    for row in cleaned_rows:\n",
    "        latex.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "    # Close table\n",
    "    latex.append(\"\\\\bottomrule\")\n",
    "    latex.append(\"\\\\end{tabular}\")\n",
    "    latex.append(\"\\\\caption{Model performance across TWX tasks}\")\n",
    "    latex.append(\"\\\\label{tab:twx-models}\")\n",
    "    latex.append(\"\\\\end{table}\")\n",
    "\n",
    "    return \"\\n\".join(latex)\n",
    "            \n",
    "all_games, game_titles  = get_all_games_per_framework(tmp, 'textworld')\n",
    "header = \" & \".join([\"Models\"] + game_titles)\n",
    "for row in formatted_rows:\n",
    "    scores = []\n",
    "    for score in all_games[row[0]]:\n",
    "        scores.append(cast_to_str(round(score * 100, 1), 1))\n",
    "    all_scores_framework = \"& \" + \" & \".join(scores)\n",
    "    header += \"\\n\" + f\"{row[0]:<{max_model_len + 2}} {all_scores_framework} \\\\\\\\\"\n",
    "\n",
    "print(generate_latex_table(header).replace(\"TW\", \"\"))\n",
    "print(len(formatted_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp.keys())\n",
    "# Practicing doing this with pandas:\n",
    "def get_avg_tokens_per_llm_framework(tmp):\n",
    "    # Using dictionary to enforce llm ordering to keep in the same order as the avg scores:\n",
    "    llm_framework_tokens = {}\n",
    "    for llm in tmp['llm'].unique():\n",
    "        llm_name_formatted = llm.split(\"/\")[-1]\n",
    "        llm_framework_tokens[llm_name_formatted] = {}\n",
    "        for framework in fws:\n",
    "            # Basing this on Marc's code\n",
    "            llm_filtered_tmp = tmp[tmp['llm'] == llm]\n",
    "            framework_filtered_tmp = llm_filtered_tmp[llm_filtered_tmp['framework'] == framework]\n",
    "            avg_tokens = framework_filtered_tmp.groupby(\"seed\")['total/Tokens'].mean().reset_index()\n",
    "            # avg_tokens = framework_filtered_tmp.groupby(\"seed\")['episode/normalized_highscore'].mean().reset_index()\n",
    "            avg_tokens.columns = [\"seeds\", \"avg_tokens_per_episode\"]\n",
    "            llm_framework_tokens[llm_name_formatted][framework] = (avg_tokens['avg_tokens_per_episode'].mean(), \n",
    "                                                         avg_tokens['avg_tokens_per_episode'].std())\n",
    "            \n",
    "    return llm_framework_tokens\n",
    "\n",
    "llm_framework_tokens = get_avg_tokens_per_llm_framework(tmp)\n",
    "all = 0\n",
    "for row in formatted_rows:\n",
    "    tokens = []\n",
    "    for framework in fws:\n",
    "        tokens.append(cast_to_str(round(llm_framework_tokens[row[0]][framework][0] * 100, 1), 1))\n",
    "    all_tokens_framework = \"& \" + \" & \".join(tokens)\n",
    "    print(f\"{row[0]:<{max_model_len + 2}} {all_tokens_framework} \\\\\\\\\")\n",
    "    all += 1\n",
    "\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the runs\n",
    "tmp = runs_df\n",
    "tmp = tmp[tmp['conversation'] == True]\n",
    "tmp1 = tmp[tmp['agent_type'] == 'zero-shot']\n",
    "tmp2 = tmp[tmp['agent_type'] == 'react']\n",
    "# tmp = tmp[tmp['agent_type'] == 'zero-shot']\n",
    "tmp = pd.concat([tmp1, tmp2])\n",
    "tmp = tmp[tmp['llm'] != 'claude-3.5-sonnet']\n",
    "tmp = tmp[tmp['game'] != 'JerichoEnvTheatre']\n",
    "tmp = tmp[~tmp['tags'].apply(lambda tags: 'without-help' in tags)]\n",
    "llm_counts = tmp[['llm', 'seed', 'game']].groupby('llm').size().reset_index(name='total_games')\n",
    "\n",
    "print(llm_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(framework_games['textworld'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = ['TWCookingLevel1', 'TWCookingLevel2', 'TWCookingLevel3', 'TWCookingLevel4', 'TWCookingLevel5', 'TWCookingLevel6', 'TWCookingLevel7', 'TWCookingLevel8', 'TWCookingLevel9', 'TWCookingLevel10']\n",
    "tmp = regen_and_filter_df()\n",
    "llm_filtered=tmp[tmp['llm'] == 'gemini-2.0-flash']\n",
    "llm_filtered = llm_filtered[llm_filtered['framework'] == 'textworld']\n",
    "# games = llm_filtered['game'].unique()\n",
    "# avg_tokens = framework_filtered_tmp.groupby(\"seed\")['total/Tokens'].mean().reset_index()\n",
    "scores = ['gemini-2.0-flash*']\n",
    "for game in games[:27]:\n",
    "    game_filtered = llm_filtered[llm_filtered['game'] == game]\n",
    "    avg_tokens = game_filtered.groupby(\"seed\")['episode/normalized_highscore'].mean().reset_index()\n",
    "    avg_tokens.columns = [\"seeds\", \"avg_tokens_per_episode\"]\n",
    "    avg_tokens['avg_tokens_per_episode']\n",
    "    scores.append(str(round(float(avg_tokens['avg_tokens_per_episode'].mean()) * 100, 1)))\n",
    "\n",
    "print(\" & \".join(scores), \"\\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this for the main website table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores_table(model_data):\n",
    "    \"\"\"\n",
    "    Generate an HTML table for model scores from an array of model data.\n",
    "    \n",
    "    Args:\n",
    "        model_data: List of lists, where each inner list contains \n",
    "                   [model_name, textworld_score, textworld_express_score, \n",
    "                    alfworld_score, scienceworld_score, jericho_score, overall_score]\n",
    "                   Scores should be provided as floats (e.g., 97.3, not \"97.3%\")\n",
    "    \n",
    "    Returns:\n",
    "        String containing the HTML table\n",
    "    \"\"\"\n",
    "    headers = [\"Model\", \"Textworld\", \"Textworld Express\", \"Alfworld\", \"Scienceworld\", \"Jericho\", \"Overall\"]\n",
    "    \n",
    "    html = '<div class=\"table-container\">\\n'\n",
    "    html += '<table class=\"model-scores\">\\n'\n",
    "    \n",
    "    # Generate table header\n",
    "    html += '    <thead>\\n'\n",
    "    html += '    <tr>\\n'\n",
    "    for header in headers:\n",
    "        html += f'        <th>{header}</th>\\n'\n",
    "    html += '    </tr>\\n'\n",
    "    html += '    </thead>\\n'\n",
    "    \n",
    "    # Generate table body\n",
    "    html += '    <tbody>\\n'\n",
    "    \n",
    "    for row in model_data:\n",
    "        model_name = row[0].replace(\"FP8\", \"\").replace(\"-03-25\", \"\")\n",
    "        scores = row[1:]\n",
    "        \n",
    "        html += '    <tr>\\n'\n",
    "        \n",
    "        # First column is the model name (bold for the top model)\n",
    "        if model_data.index(row) == 0:\n",
    "            html += f'        <td><strong>{model_name}</strong></td>\\n'\n",
    "        else:\n",
    "            html += f'        <td>{model_name}</td>\\n'\n",
    "        \n",
    "        # Add scores with percentage format\n",
    "        for score in scores:\n",
    "            html += f'        <td>{float(score):.1f}%</td>\\n'\n",
    "        \n",
    "        html += '    </tr>\\n'\n",
    "    \n",
    "    html += '    </tbody>\\n'\n",
    "    html += '</table>\\n'\n",
    "    html += '</div>\\n'\n",
    "    \n",
    "    return html\n",
    "\n",
    "html_table = generate_model_scores_table(formatted_rows)\n",
    "\n",
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "base_path = cwd.split(\"assets/figs\")[0]\n",
    "new_path = base_path + \"_includes/\"\n",
    "with open(new_path + \"table.md\", \"w\") as f:\n",
    "    f.write(html_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_org = {\n",
    "    \"claude-3.7-sonnet\": \"Anthropic\",\n",
    "    \"claude-3.5-sonnet-latest\": \"Anthropic\",\n",
    "    \"gemini-2.5-pro-preview-03-25*\": \"Google\",\n",
    "    \"o1\": \"Anthropic\",\n",
    "    \"gpt-4o\": \"OpenAI\",\n",
    "    \"claude-3.5-haiku\": \"Anthropic\",\n",
    "    \"Llama-3.1-405B-Instruct\": \"Meta\",\n",
    "    \"gemini-2.0-flash\": \"Google\",\n",
    "    \"Llama-3.3-70B-Instruct\": \"Meta\",\n",
    "    \"Llama-3.1-70B-Instruct\": \"Meta\",\n",
    "    \"Qwen2.5-72B-Instruct\": \"Alibaba\",\n",
    "    \"Mistral-Large-Instruct-2407\": \"Mistral AI\",\n",
    "    \"gpt-4o-mini\": \"OpenAI\",\n",
    "    \"Llama-4-Scout-17B-16E-Instruct\": \"Meta\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\": \"Meta\",\n",
    "    \"Mistral-Small-Instruct-2409\": \"Mistral AI\",\n",
    "    \"Llama-3.1-8B-Instruct\": \"Meta\",\n",
    "    \"DeepSeek-R1\": \"DeepSeek AI\",\n",
    "    \"Qwen2.5-7B-Instruct\": \"Alibaba\",\n",
    "    \"Llama-3.2-3B-Instruct\": \"Meta\",\n",
    "    \"phi-4\": \"Microsoft\",\n",
    "    \"Mistral-Small-24B-Instruct-2501\": \"Mistral AI\",\n",
    "    \"DeepSeek-R1-Distill-Llama-70B\": \"DeepSeek AI\",\n",
    "    \"Ministral-8B-Instruct-2410\": \"Mistral AI\",\n",
    "    \"Mistral-Small-3.1-24B-Instruct-2503\": \"Mistral AI\",\n",
    "    \"Mixtral-8x22B-Instruct-v0.1\": \"Mistral AI\",\n",
    "    \"Llama-3.2-1B-Instruct\": \"Meta\",\n",
    "    \"Phi-3-mini-128k-instruct\": \"Microsoft\",\n",
    "    \"Phi-3.5-MoE-instruct\": \"Microsoft\",\n",
    "    \"Phi-4-mini-instruct\": \"Microsoft\",\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": \"Mistral AI\",\n",
    "    \"Phi-3.5-mini-instruct\": \"Microsoft\",\n",
    "    \"Phi-3-medium-128k-instruct\": \"Microsoft\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores_table_simplified(model_data):\n",
    "    \"\"\"\n",
    "    Generate a simplified HTML table for model scores with:\n",
    "    1. Rank number\n",
    "    2. Model name\n",
    "    3. Overall score\n",
    "    4. Whether it's a reasoning model\n",
    "    \n",
    "    Args:\n",
    "        model_data: List of lists, where each inner list contains \n",
    "                   [model_name, textworld_score, textworld_express_score, \n",
    "                    alfworld_score, scienceworld_score, jericho_score, overall_score]\n",
    "    \n",
    "    Returns:\n",
    "        String containing the HTML table\n",
    "    \"\"\"\n",
    "    headers = [\"Rank\", \"Model\", \"Organization\", \"Model Type\", \"TALES Score\"]\n",
    "    \n",
    "    # Helper function to determine if a model uses reasoning\n",
    "    # This is a placeholder - you'll need to fill in actual data\n",
    "\n",
    "    reasoning_models = ['DeepSeek-R1', \"claude-3.7-sonnet\", 'o1', 'DeepSeek-R1-Distill-Llama-70B']\n",
    "    \n",
    "    html = '<div class=\"table-container\">\\n'\n",
    "    html += '<table class=\"model-scores simplified-scores\">\\n'\n",
    "    \n",
    "    # Generate table header\n",
    "    html += '    <thead>\\n'\n",
    "    html += '    <tr>\\n'\n",
    "    for header in headers:\n",
    "        html += f'        <th>{header}</th>\\n'\n",
    "    html += '    </tr>\\n'\n",
    "    html += '    </thead>\\n'\n",
    "    \n",
    "    # Generate table body\n",
    "    html += '    <tbody>\\n'\n",
    "    \n",
    "    # Model data should already be sorted by overall score (highest first)\n",
    "    for i, row in enumerate(model_data):\n",
    "        rank = i + 1\n",
    "        model_name = row[0].replace(\"-FP8\", \"\").replace(\"-03-25\", \"\")\n",
    "        # The overall score is the last element in the row\n",
    "        overall_score = row[-1]\n",
    "        if model_name in reasoning_models:\n",
    "            reasoning = \"Reasoning\"\n",
    "        else:\n",
    "            reasoning = \"Non-reasoning\"\n",
    "        \n",
    "        html += '    <tr>\\n'\n",
    "        \n",
    "         # Add rank\n",
    "        html += f'        <td>{rank}</td>\\n'\n",
    "        \n",
    "        # Model name (bold for the top model)\n",
    "        # if i == 0:\n",
    "        html += f'        <td><strong>{model_name}</strong></td>\\n'\n",
    "        # else:\n",
    "        #     html += f'        <td>{model_name}</td>\\n'\n",
    "\n",
    "          # Add rank\n",
    "        html += f'        <td>{model_org[row[0]]}</td>\\n'\n",
    "        \n",
    "        # Reasoning model indicator\n",
    "        html += f'        <td>{reasoning}</td>\\n'\n",
    "\n",
    "        # Overall score with percentage format\n",
    "        html += f'        <td>{float(overall_score):.1f}%</td>\\n'\n",
    "        \n",
    "        \n",
    "        html += '    </tr>\\n'\n",
    "    \n",
    "    html += '    </tbody>\\n'\n",
    "    html += '</table>\\n'\n",
    "    html += '</div>\\n'\n",
    "    \n",
    "    return html\n",
    "\n",
    "html_table = generate_model_scores_table_simplified(formatted_rows)\n",
    "\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "with open(new_path + \"simple_table.md\", \"w\") as f:\n",
    "    f.write(html_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def make_bar_chart(llms, frameworks, error_bars = True, name_replacement_dict = {}, divider_idx = -1, plot_name = \"bar_chart\"):\n",
    "    framework_indicies = []\n",
    "    for framework in frameworks:\n",
    "        if \":\" in framework:\n",
    "            # If there is a : in the framework, get it and split to get the names of all of the games that are to be graphed for that framework\n",
    "            all_games = framework.split(\":\")[1].split(\",\")\n",
    "            for game in all_games:\n",
    "                assert game in framework_games[framework.split(\":\")[0]], f\"Game {game} not in frameworks list\"\n",
    "            framework_indicies.append((fws.index(framework.split(\":\")[0]) + 1, all_games))\n",
    "        else:\n",
    "            framework_indicies.append(fws.index(framework) + 1) # +1 to deal with model name offset\n",
    "    # framework_indicies = sorted(framework_indicies)\n",
    "\n",
    "    # Filter the scores_per_framework array based on the llms and frameworks\n",
    "    # Filter the stds_per_framework array based on the llms and frameworks\n",
    "    filtered_scores_per_framework = []\n",
    "    filtered_stds_per_framework = []\n",
    "    for i, row in enumerate(scores_per_framework):\n",
    "        if row[0] not in llms: continue\n",
    "        new_row_scores = [row[0]]\n",
    "        new_row_stds = []\n",
    "        for idx in framework_indicies: \n",
    "            if type(idx) == tuple:\n",
    "                # If the index is a tuple, it means we have multiple games for that framework\n",
    "                # Get the average score and std for those games\n",
    "                game_scores = []\n",
    "                game_stds = []\n",
    "                for game in idx[1]:\n",
    "                    framework = fws[idx[0] - 1]\n",
    "                    game_scores_tokens = llm_framework_scores[row[0]][framework][game]\n",
    "                    game_scores = []\n",
    "                    for score_token in game_scores_tokens.values():\n",
    "                        game_scores.append(score_token[0])\n",
    "\n",
    "                    new_row_scores.append(np.mean(game_scores))\n",
    "                    new_row_stds.append(np.std(game_scores, ddof=1))\n",
    "            else:\n",
    "                new_row_scores.append(row[idx])\n",
    "\n",
    "                # new_row_stds.append(stds_per_framework[i][idx])\n",
    "                # Calculate the std on a per-run basis\n",
    "                # Get the games\n",
    "                framework = fws[idx - 1]\n",
    "                all_games_scores_tokens = llm_framework_scores[row[0]][framework]\n",
    "                seeds_scores = {}\n",
    "                # Get the averages per seed\n",
    "                for _, seeds in all_games_scores_tokens.items():\n",
    "                    for seed, score_tokens in seeds.items():\n",
    "                        if seed not in seeds_scores.keys():\n",
    "                            seeds_scores[seed] = [score_tokens[0]]\n",
    "                        else:\n",
    "                            seeds_scores[seed].append(score_tokens[0])\n",
    "                summed_scores_per_seeds = []\n",
    "                for seed, scores in seeds_scores.items():\n",
    "                    summed_scores_per_seeds.append(np.mean(scores))\n",
    "                new_row_stds.append(np.std(summed_scores_per_seeds, ddof=1))\n",
    "\n",
    "\n",
    "        filtered_scores_per_framework.append(new_row_scores)\n",
    "        filtered_stds_per_framework.append(new_row_stds)\n",
    "\n",
    "\n",
    "    framework_names = []\n",
    "    for framework in frameworks:\n",
    "        if \":\" not in framework:\n",
    "            framework_names.append(framework)\n",
    "        else:\n",
    "            # Get the atcual framework name\n",
    "            framework_name = framework.split(\":\")[0]\n",
    "            # Then add all the games\n",
    "            for game in framework.split(\":\")[1].split(\",\"):\n",
    "                if game in name_replacement_dict.keys():\n",
    "                    framework_names.append(f\"{name_replacement_dict[game]}\")\n",
    "                else:\n",
    "                    framework_names.append(game)\n",
    "\n",
    "    transposed_std_per_framework = np.transpose(filtered_stds_per_framework)\n",
    "    llm_names = llms\n",
    "    framework_scores_sets = []\n",
    "    framework_std_sets = []\n",
    "    for i, row in enumerate(np.transpose(filtered_scores_per_framework)[1:]):\n",
    "        framework_scores_sets.append([float(score) * 100 for score in row])\n",
    "        framework_std_sets.append([float(std) * 100 for std in transposed_std_per_framework[i]])\n",
    "    # Sort scores, standard deviations, and LLM names for each framework\n",
    "    sorted_framework_scores_sets = []\n",
    "    sorted_framework_std_sets = []\n",
    "    sorted_llm_names_sets = []\n",
    "\n",
    "    # For each framework, create a mapping from LLM to its color to maintain color consistency\n",
    "    llm_to_color = {llm: plt.cm.tab10(i % 10) for i, llm in enumerate(llm_names)}\n",
    "\n",
    "    for framework_name, framework_scores, framework_stds in zip(framework_names, framework_scores_sets, framework_std_sets):\n",
    "        # Pair scores with their corresponding LLM names and standard deviations\n",
    "        paired_data = list(zip(framework_scores, framework_stds, llm_names))\n",
    "        \n",
    "        # Sort the pairs by score\n",
    "        paired_data.sort(key=lambda pair: pair[0])\n",
    "        \n",
    "        # Unzip the sorted pairs back into separate lists\n",
    "        sorted_scores, sorted_stds, sorted_llms = zip(*paired_data)\n",
    "        sorted_framework_scores_sets.append(sorted_scores)\n",
    "        sorted_framework_std_sets.append(sorted_stds)\n",
    "        sorted_llm_names_sets.append(sorted_llms)\n",
    "\n",
    "    # Create a bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Bar width and positions\n",
    "    bar_width = 0.8 / len(llm_names)  # Dynamically adjust bar width based on the number of LLMs\n",
    "    x = range(len(framework_names))  # X-axis positions for frameworks\n",
    "\n",
    "    # Create a set to track which LLMs have been added to the legend\n",
    "    legend_added = set()\n",
    "\n",
    "    # Plot bars for each framework\n",
    "    for i, (framework_name, sorted_scores, sorted_stds, sorted_llms) in enumerate(zip(\n",
    "        framework_names, sorted_framework_scores_sets, sorted_framework_std_sets, sorted_llm_names_sets)):\n",
    "        for j, (llm_name, score, std) in enumerate(zip(sorted_llms, sorted_scores, sorted_stds)):\n",
    "            # Add to legend only if this LLM hasn't been added yet\n",
    "            if llm_name not in legend_added:\n",
    "                label = llm_name\n",
    "                legend_added.add(llm_name)\n",
    "            else:\n",
    "                label = \"\"\n",
    "            \n",
    "            # Calculate x position for the bar\n",
    "            x_pos = x[i] + j * bar_width - (len(sorted_llms) * bar_width) / 2\n",
    "            \n",
    "            # Draw the bar\n",
    "            ax.bar(\n",
    "                x_pos,\n",
    "                score,\n",
    "                width=bar_width,\n",
    "                label=label,\n",
    "                color=llm_to_color[llm_name],  # Use consistent colors for each LLM\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            if error_bars:\n",
    "                # Add error bars\n",
    "                ax.errorbar(\n",
    "                    x_pos,\n",
    "                    score,\n",
    "                    yerr=std,\n",
    "                    fmt='none',  # No line connecting error bars\n",
    "                    ecolor='black',  # Color of error bars\n",
    "                    capsize=5,  # Size of caps at the end of error bars\n",
    "                    capthick=1.5,  # Thickness of caps\n",
    "                    elinewidth=1.5,  # Thickness of error bar lines\n",
    "                    alpha = 0.2\n",
    "                )\n",
    "\n",
    "    # Set x-axis labels and ticks\n",
    "    ax.set_xticks(x)  # Center the ticks\n",
    "    ax.set_xticklabels(framework_names)  # Set framework names as x-axis labels\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel('Highest Percent of Score Achieved')\n",
    "    ax.set_xlabel('Games(Simon Says, Zork1) and Frameworks(Alfworld, Scienceworld)')\n",
    "    ax.set_title('Highest Percent of Score Achieved for Top LLMs', fontweight='bold')\n",
    "\n",
    "    # Find index of 'scienceworld' and 'jericho' to place the dividing line\n",
    "    try:\n",
    "        # Get the index of the right most snythetic framework\n",
    "        # non_synth_frameworks = ['jericho']\n",
    "        # right_most_idx = 0\n",
    "        # for framework_name in framework_names:\n",
    "        #     print(framework_name)\n",
    "        #     idx = framework_names.index(framework_name)\n",
    "        #     print(idx)\n",
    "        #     if idx > right_most_idx:\n",
    "        #         non_synth = False\n",
    "        #         for non_synth_framework in non_synth_frameworks:\n",
    "        #             if non_synth_framework in framework_name:\n",
    "        #                 non_synth = True\n",
    "        #                 break\n",
    "        #         if not non_synth:\n",
    "        #             print(\"Updated\")\n",
    "        #             right_most_idx = idx\n",
    "\n",
    "        # jer_idx = 0\n",
    "        # for framework_name in framework_names:\n",
    "        #     idx = framework_names.index(framework_name)\n",
    "        #     if 'jericho' in framework_name:\n",
    "        #         jer_idx = idx\n",
    "        #         break\n",
    "\n",
    "        jer_idx = divider_idx\n",
    "        right_most_idx = jer_idx - 1\n",
    "\n",
    "        # Make sure they're adjacent for the dividing line to make sense\n",
    "        if jer_idx == right_most_idx + 1:\n",
    "            # Calculate position between scienceworld and jericho\n",
    "            divider_x = (x[right_most_idx] + x[jer_idx]) / 2\n",
    "            \n",
    "            # Get y-axis limits for the line height\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            line_height = ymax\n",
    "            \n",
    "            # Draw vertical divider line\n",
    "            ax.axvline(x=divider_x, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Add \"synthetic\" and \"human-written\" labels\n",
    "            y_text_pos = ymax * .9  # Position just above the top of the plot\n",
    "            \n",
    "            # Calculate positions for synthetic/human-written labels\n",
    "            # Adjust these offsets to position the text properly\n",
    "            spacing = 0.03\n",
    "            synthetic_x = divider_x - spacing\n",
    "            human_x = divider_x + spacing\n",
    "            \n",
    "            ax.text(synthetic_x, y_text_pos, 'Synthetic', ha='right', va='bottom', \n",
    "                   fontsize=12, fontweight='bold', fontfamily = 'monospace')\n",
    "            ax.text(human_x, y_text_pos, 'Human-written', ha='left', va='bottom', \n",
    "                   fontsize=12, fontweight='bold', fontstyle='italic', fontfamily='serif')\n",
    "            \n",
    "            # Adjust top margin to make room for the labels\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "    except ValueError:\n",
    "        # If either framework isn't in the list, don't add the divider\n",
    "        print(\"Note: Could not add divider - scienceworld and jericho must both be in frameworks list\")\n",
    "\n",
    "    # Add legend - placed outside to ensure it's visible and complete\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles, labels,\n",
    "        title=\"\",\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, .02),  # Move the legend closer to the subplots\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        fontsize='medium',  # Set the font size for the legend text\n",
    "        prop={'weight': 'bold'} \n",
    "    )\n",
    "\n",
    "    # Adjust layout\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(plot_name + '.png')\n",
    "    plt.savefig(plot_name + \".png\", bbox_inches='tight', pad_inches = 0.5)\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "\n",
    "downsampled_llms = ['meta-llama/Llama-3.1-70B-Instruct',\n",
    " 'meta-llama/Llama-3.1-405B-Instruct',\n",
    " 'gpt-4o', \n",
    "#  'Qwen/Qwen2.5-72B-Instruct',\n",
    " 'gpt-4o-mini',\n",
    " 'claude-3.5-sonnet-latest',\n",
    " 'claude-3.5-haiku',\n",
    "#  'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
    " 'meta-llama/Llama-3.3-70B-Instruct', \n",
    "#  'o3-mini', \n",
    " 'o1', \n",
    " 'claude-3.7-sonnet']\n",
    "name_replacement = {'TWXSimonSaysWithMemory100': 'SimonSays with Memory (100)', 'JerichoEnvZork1' : 'Zork1'}\n",
    "downsampled_frameworks = ['textworld_express:TWXSimonSaysWithMemory100', 'alfworld', 'scienceworld', 'jericho:JerichoEnvZork1']\n",
    "make_bar_chart(llms = downsampled_llms, \n",
    "                frameworks = downsampled_frameworks,\n",
    "                error_bars = True, \n",
    "                name_replacement_dict = name_replacement,\n",
    "                divider_idx = 3,\n",
    "                plot_name = \"all_framework_scores\")\n",
    "                # frameworks = fws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_chart_horizontal(llms, frameworks, error_bars = True, name_replacement_dict = {}, divider_idx = -1, height_per_graph = 0.8):\n",
    "    framework_indicies = []\n",
    "    for framework in frameworks:\n",
    "        if \":\" in framework:\n",
    "            # If there is a : in the framework, get it and split to get the names of all of the games that are to be graphed for that framework\n",
    "            all_games = framework.split(\":\")[1].split(\",\")\n",
    "            for game in all_games:\n",
    "                assert game in framework_games[framework.split(\":\")[0]], f\"Game {game} not in frameworks list\"\n",
    "            framework_indicies.append((fws.index(framework.split(\":\")[0]) + 1, all_games))\n",
    "        else:\n",
    "            framework_indicies.append(fws.index(framework) + 1) # +1 to deal with model name offset\n",
    "\n",
    "    # Filter the scores_per_framework array based on the llms and frameworks\n",
    "    # Filter the stds_per_framework array based on the llms and frameworks\n",
    "    filtered_scores_per_framework = []\n",
    "    filtered_stds_per_framework = []\n",
    "    for i, row in enumerate(scores_per_framework):\n",
    "        if row[0] not in llms: continue\n",
    "        new_row_scores = [row[0]]\n",
    "        new_row_stds = []\n",
    "        for idx in framework_indicies: \n",
    "            if type(idx) == tuple:\n",
    "                # If the index is a tuple, it means we have multiple games for that framework\n",
    "                # Get the average score and std for those games\n",
    "                game_scores = []\n",
    "                game_stds = []\n",
    "                for game in idx[1]:\n",
    "                    framework = fws[idx[0] - 1]\n",
    "                    game_scores_tokens = llm_framework_scores[row[0]][framework][game]\n",
    "                    game_scores = []\n",
    "                    for score_token in game_scores_tokens.values():\n",
    "                        game_scores.append(score_token[0])\n",
    "\n",
    "                    new_row_scores.append(np.mean(game_scores))\n",
    "                    new_row_stds.append(np.std(game_scores))\n",
    "            else:\n",
    "                new_row_scores.append(row[idx])\n",
    "                # new_row_stds.append(stds_per_framework[i][idx])\n",
    "                # new_row_stds.append(stds_per_framework[i][idx])\n",
    "                # Calculate the std on a per-run basis\n",
    "                # Get the games\n",
    "                framework = fws[idx - 1]\n",
    "                all_games_scores_tokens = llm_framework_scores[row[0]][framework]\n",
    "                seeds_scores = {}\n",
    "                # Get the averages per seed\n",
    "                for _, seeds in all_games_scores_tokens.items():\n",
    "                    for seed, score_tokens in seeds.items():\n",
    "                        if seed not in seeds_scores.keys():\n",
    "                            seeds_scores[seed] = [score_tokens[0]]\n",
    "                        else:\n",
    "                            seeds_scores[seed].append(score_tokens[0])\n",
    "                summed_scores_per_seeds = []\n",
    "                for seed, scores in seeds_scores.items():\n",
    "                    summed_scores_per_seeds.append(np.mean(scores))\n",
    "                new_row_stds.append(np.std(summed_scores_per_seeds, ddof=1))\n",
    "\n",
    "        filtered_scores_per_framework.append(new_row_scores)\n",
    "        filtered_stds_per_framework.append(new_row_stds)\n",
    "\n",
    "    framework_names = []\n",
    "    for framework in frameworks:\n",
    "        if \":\" not in framework:\n",
    "            framework_names.append(framework)\n",
    "        else:\n",
    "            # Get the atcual framework name\n",
    "            framework_name = framework.split(\":\")[0]\n",
    "            # Then add all the games\n",
    "            for game in framework.split(\":\")[1].split(\",\"):\n",
    "                if game in name_replacement_dict.keys():\n",
    "                    framework_names.append(f\"{name_replacement_dict[game]}\")\n",
    "                else:\n",
    "                    framework_names.append(game)\n",
    "\n",
    "\n",
    "    transposed_std_per_framework = np.transpose(filtered_stds_per_framework)\n",
    "    llm_names = llms\n",
    "    framework_scores_sets = []\n",
    "    framework_std_sets = []\n",
    "    for i, row in enumerate(np.transpose(filtered_scores_per_framework)[1:]):\n",
    "        framework_scores_sets.append([float(score) for score in row])\n",
    "        framework_std_sets.append([float(std) for std in transposed_std_per_framework[i]])\n",
    "    # Sort scores, standard deviations, and LLM names for each framework\n",
    "    sorted_framework_scores_sets = []\n",
    "    sorted_framework_std_sets = []\n",
    "    sorted_llm_names_sets = []\n",
    "\n",
    "    # For each framework, create a mapping from LLM to its color to maintain color consistency\n",
    "    llm_to_color = {llm: plt.cm.tab10(i % 10) for i, llm in enumerate(llm_names)}\n",
    "\n",
    "    for framework_name, framework_scores, framework_stds in zip(framework_names, framework_scores_sets, framework_std_sets):\n",
    "        # Pair scores with their corresponding LLM names and standard deviations\n",
    "        paired_data = list(zip(framework_scores, framework_stds, llm_names))\n",
    "        \n",
    "        # Sort the pairs by score\n",
    "        paired_data.sort(key=lambda pair: pair[0])\n",
    "        \n",
    "        # Unzip the sorted pairs back into separate lists\n",
    "        sorted_scores, sorted_stds, sorted_llms = zip(*paired_data)\n",
    "        sorted_framework_scores_sets.append(sorted_scores)\n",
    "        sorted_framework_std_sets.append(sorted_stds)\n",
    "        sorted_llm_names_sets.append(sorted_llms)\n",
    "\n",
    "    total_items = len(framework_names)\n",
    "    height_per_item = height_per_graph  # inches per game/framework\n",
    "    fig_height = max(8, total_items * height_per_item)\n",
    "\n",
    "    # Create a bar chart with extra space on right for legend\n",
    "    fig, ax = plt.subplots(figsize=(14, fig_height))  # Increased width to accommodate legend\n",
    "\n",
    "    # Bar width and positions\n",
    "    bar_width = 0.8 / len(llm_names)  # Dynamically adjust bar width based on the number of LLMs\n",
    "    y = range(len(framework_names))  # Y-axis positions for frameworks\n",
    "\n",
    "    # Create a set to track which LLMs have been added to the legend\n",
    "    legend_added = set()\n",
    "\n",
    "    # Plot bars for each framework\n",
    "    for i, (framework_name, sorted_scores, sorted_stds, sorted_llms) in enumerate(zip(\n",
    "        framework_names, sorted_framework_scores_sets, sorted_framework_std_sets, sorted_llm_names_sets)):\n",
    "        for j, (llm_name, score, std) in enumerate(zip(sorted_llms, sorted_scores, sorted_stds)):\n",
    "            # Add to legend only if this LLM hasn't been added yet\n",
    "            if llm_name not in legend_added:\n",
    "                label = llm_name.split('/')[-1] if '/' in llm_name else llm_name  # Use shorter names\n",
    "                legend_added.add(llm_name)\n",
    "            else:\n",
    "                label = \"\"\n",
    "            \n",
    "            # Calculate y position for the bar\n",
    "            y_pos = y[i] + j * bar_width - (len(sorted_llms) * bar_width) / 2\n",
    "            \n",
    "            # Draw the bar\n",
    "            ax.barh(\n",
    "                y_pos,\n",
    "                score,\n",
    "                height=bar_width,\n",
    "                label=label,\n",
    "                color=llm_to_color[llm_name],\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            if error_bars:\n",
    "                # Add error bars\n",
    "                ax.errorbar(\n",
    "                    score,\n",
    "                    y_pos,\n",
    "                    xerr=std,\n",
    "                    fmt='none',\n",
    "                    ecolor='black',\n",
    "                    capsize=5,\n",
    "                    capthick=1.5,\n",
    "                    elinewidth=1.5\n",
    "                )\n",
    "\n",
    "    # Set y-axis labels and ticks with 60-degree rotation\n",
    "    ax.set_yticks(y)\n",
    "    # ax.set_yticklabels(framework_names, rotation=60, ha='right', va='center')  # 60-degree angle\n",
    "    ax.set_yticklabels(framework_names, ha='right', va='center')  # 60-degree angle\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Highest Normalized High Score')\n",
    "    ax.set_ylabel('Frameworks')\n",
    "    ax.set_title('Highest Normalized High Score Achieved by Each LLM Across Frameworks', fontweight='bold')\n",
    "    ax.set_xlim(0, 1.2)\n",
    "\n",
    "    # Find index of 'scienceworld' and 'jericho' to place the dividing line\n",
    "    try:\n",
    "        # Get the index of the right most synthetic framework\n",
    "        # non_synth_frameworks = ['jericho']\n",
    "        # right_most_idx = 0\n",
    "        # for framework_name in framework_names:\n",
    "        #     idx = framework_names.index(framework_name)\n",
    "        #     if idx > right_most_idx:\n",
    "        #         non_synth = False\n",
    "        #         for non_synth_framework in non_synth_frameworks:\n",
    "        #             if non_synth_framework in framework_name:\n",
    "        #                 non_synth = True\n",
    "        #                 break\n",
    "        #         if not non_synth:\n",
    "        #             right_most_idx = idx\n",
    "\n",
    "        # jer_idx = 0\n",
    "        # for framework_name in framework_names:\n",
    "        #     idx = framework_names.index(framework_name)\n",
    "        #     if 'jericho' in framework_name:\n",
    "        #         jer_idx = idx\n",
    "        #         break\n",
    "\n",
    "        jer_idx = divider_idx\n",
    "        right_most_idx = jer_idx - 1\n",
    "\n",
    "        # Make sure they're adjacent for the dividing line to make sense\n",
    "        if jer_idx == right_most_idx + 1:\n",
    "            # Calculate position between scienceworld and jericho\n",
    "            divider_y = (y[right_most_idx] + y[jer_idx]) / 2\n",
    "            \n",
    "            # Get x-axis limits for the line width\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            \n",
    "            # Draw horizontal divider line\n",
    "            ax.axhline(y=divider_y, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Add \"synthetic\" and \"human-written\" labels\n",
    "            x_text_pos = xmax * 0.9\n",
    "            \n",
    "            # Calculate positions for synthetic/human-written labels\n",
    "            spacing = 0.2\n",
    "            synthetic_y = divider_y - spacing\n",
    "            human_y = divider_y + spacing\n",
    "            \n",
    "            # Text positioned to the right of the dividing line\n",
    "            ax.text(x_text_pos, synthetic_y, 'Synthetic', ha='right', va='center', \n",
    "                   fontsize=12, fontweight='bold', fontfamily = 'monospace')\n",
    "            ax.text(x_text_pos, human_y, 'Human-written', ha='right', va='center', \n",
    "                   fontsize=12, fontweight='bold', fontstyle='italic', fontfamily='serif')\n",
    "    except ValueError:\n",
    "        # If either framework isn't in the list, don't add the divider\n",
    "        print(\"Note: Could not add divider - scienceworld and jericho must both be in frameworks list\")\n",
    "\n",
    "    # Add legend to the right side in a single column\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    # Sort to ensure they appear in the original order\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    sorted_labels = []\n",
    "    for llm in llms:\n",
    "        short_name = llm.split('/')[-1] if '/' in llm else llm\n",
    "        if short_name in labels:\n",
    "            sorted_labels.append(short_name)\n",
    "        \n",
    "    sorted_handles = [by_label[label] for label in sorted_labels if label in by_label]\n",
    "    \n",
    "    # Place legend on the right side in a single column\n",
    "    fig.legend(\n",
    "        sorted_handles, sorted_labels,\n",
    "        title=\"Language Models\",\n",
    "        loc='center right',\n",
    "        bbox_to_anchor=(1.15, 0.5),  # Position to the right of the plot\n",
    "        ncol=1,  # Single column\n",
    "        frameon=False,\n",
    "        fontsize='small',\n",
    "        prop={'weight': 'bold'}\n",
    "    )\n",
    "\n",
    "    # Adjust layout - reduce right padding to accommodate the legend\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)  # Leave space on the right for the legend\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "framework_games['textworld_express']\n",
    "all_frameworks_by_games = []\n",
    "for framework, games in framework_games.items():\n",
    "    all_frameworks_by_games.append(framework + \":\" + \",\".join(games))\n",
    "\n",
    "make_bar_chart_horizontal(llms = downsampled_llms, \n",
    "                # frameworks = all_frameworks_by_games,\n",
    "                frameworks = fws,\n",
    "                error_bars = True, \n",
    "                name_replacement_dict = name_replacement,\n",
    "                divider_idx = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make horizontal bar charts per game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_game_bar_charts(llms, framework, games=None, height_per_game = 0.5, plot_name = \"placeholder\"):\n",
    "    \"\"\"\n",
    "    Creates horizontal bar charts showing LLM performance for each game in a specific framework.\n",
    "    \n",
    "    Args:\n",
    "        llms: List of LLM names to include\n",
    "        framework: The specific framework to analyze\n",
    "        games: Optional list of games to include (if None, will use all games in the framework)\n",
    "    \"\"\"\n",
    "    # If no games specified, get all games for the framework\n",
    "    if games is None:\n",
    "        games = framework_games[framework]\n",
    "    \n",
    "    # Create a color mapping for LLMs for consistency\n",
    "    llm_to_color = {llm: plt.cm.tab10(i % 10) for i, llm in enumerate(llms)}\n",
    "    \n",
    "    # Create a figure with subplots - one per game\n",
    "    n_games = len(games)\n",
    "    fig_height = n_games * height_per_game  # Dynamic height based on number of games\n",
    "    fig, axes = plt.subplots(n_games, 1, figsize=(12, fig_height), sharex=True, \n",
    "                           constrained_layout=False)\n",
    "    \n",
    "    # If there's only one game, make sure axes is still iterable\n",
    "    if n_games == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Keep track of the maximum score to set consistent x-axis limits\n",
    "    max_score = 0\n",
    "    \n",
    "    # Process each game\n",
    "    for game_idx, game in enumerate(games):\n",
    "        ax = axes[game_idx]\n",
    "        \n",
    "        # Collect scores and stds for this game across all LLMs\n",
    "        game_scores = []\n",
    "        game_stds = []\n",
    "        valid_llms = []\n",
    "        \n",
    "        for llm in llms:\n",
    "            try:\n",
    "                # Get all seeds for this game\n",
    "                seeds = llm_framework_scores[llm][framework][game]\n",
    "                \n",
    "                # Extract scores from all seeds\n",
    "                scores = [seed_data[0] for seed_data in seeds.values()]\n",
    "                \n",
    "                # Calculate mean and std\n",
    "                mean_score = np.mean(scores)\n",
    "                std_score = np.std(scores)\n",
    "                \n",
    "                game_scores.append(mean_score)\n",
    "                game_stds.append(std_score)\n",
    "                valid_llms.append(llm)\n",
    "                \n",
    "                # Update max score for x-axis scaling\n",
    "                if mean_score + std_score > max_score:\n",
    "                    max_score = mean_score + std_score\n",
    "                    \n",
    "            except (KeyError, ValueError):\n",
    "                # Skip if LLM doesn't have data for this game\n",
    "                continue\n",
    "        \n",
    "        # Sort scores, stds, and LLMs together (highest score first)\n",
    "        sorted_data = sorted(zip(game_scores, game_stds, valid_llms), key=lambda x: x[0], reverse=False)\n",
    "        \n",
    "        # Unpack the sorted data\n",
    "        if sorted_data:  # Check if there's any data\n",
    "            sorted_scores, sorted_stds, sorted_llms = zip(*sorted_data)\n",
    "        else:\n",
    "            # Skip this game if no data\n",
    "            ax.set_title(f\"{game} (No data)\")\n",
    "            continue\n",
    "            \n",
    "        # Set y positions for bars\n",
    "        y_positions = range(len(sorted_llms))\n",
    "        \n",
    "        # Draw horizontal bars\n",
    "        for i, (llm, score, std) in enumerate(zip(sorted_llms, sorted_scores, sorted_stds)):\n",
    "            # Draw the bar\n",
    "            ax.barh(\n",
    "                i,\n",
    "                score,\n",
    "                height=1.0,\n",
    "                label=llm if game_idx == 0 and i == 0 else \"\",  # Only add to legend once\n",
    "                color=llm_to_color[llm],\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            # Add error bars\n",
    "            ax.errorbar(\n",
    "                score,\n",
    "                i,\n",
    "                xerr=std,\n",
    "                fmt='none',\n",
    "                ecolor='black',\n",
    "                capsize=3,\n",
    "                capthick=1,\n",
    "                elinewidth=1\n",
    "            )\n",
    "            \n",
    "            # Add score value at the end of each bar\n",
    "            ax.text(\n",
    "                score + std + 0.01,  # Slight offset from end of error bar\n",
    "                i,\n",
    "                f\"{score:.3f}{std:.3f}\",\n",
    "                va='center',\n",
    "                fontsize=8\n",
    "            )\n",
    "        \n",
    "        # Set y-ticks to be the LLM names\n",
    "        ax.set_yticks(y_positions)\n",
    "        shortened_llm_names = [llm.split('/')[-1] for llm in sorted_llms]  # Get just the model name without path\n",
    "        ax.set_yticklabels(shortened_llm_names)\n",
    "        \n",
    "        # Set title for this subplot\n",
    "        ax.set_title(game)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Only show x-label on the bottom plot\n",
    "        if game_idx == n_games - 1:\n",
    "            ax.set_xlabel('Normalized Score')\n",
    "    \n",
    "    # Set consistent x-axis limits\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0, max_score * 1.2)  # Add 20% padding\n",
    "    \n",
    "    \n",
    "    # plt.suptitle(f'Performance Across Games in {framework.capitalize()}', fontsize=16, y=0.99)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(\n",
    "        hspace=0.1,       # Space between subplots (reduce this if needed)\n",
    "        # top=2.0,         # Top margin (increase this to move title down)\n",
    "        bottom=0.05,      # Bottom margin\n",
    "        left=0.1,         # Left margin\n",
    "        right=0.9         # Right margin\n",
    "    )\n",
    "    \n",
    "    plt.savefig(plot_name + '.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "# Example usage for one framework\n",
    "framework_to_analyze = 'scienceworld'  # You can replace with any framework from fws\n",
    "selected_games = framework_games[framework_to_analyze]  # Limit to first 5 games to avoid overcrowding\n",
    "framework_save_title = {'textworld_express': 'textworld_express_all_games', 'textworld': 'textworld_all_games', 'alfworld': 'alfworld_all_games', 'scienceworld': 'scienceworld_all_games', 'jericho': 'jericho_all_games'}\n",
    "# Create the bar charts\n",
    "\n",
    "for framework, title in framework_save_title.items():\n",
    "    make_game_bar_charts(\n",
    "        llms=downsampled_llms, \n",
    "        framework=framework,\n",
    "        games=framework_games[framework],\n",
    "        height_per_game = 4.0,\n",
    "        plot_name=title\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_game_bar_charts(llms, framework, games=None, height_per_game=0.5, plot_name=\"placeholder\"):\n",
    "    \"\"\"\n",
    "    Creates horizontal bar charts showing LLM performance for each game in a specific framework.\n",
    "    \n",
    "    Args:\n",
    "        llms: List of LLM names to include\n",
    "        framework: The specific framework to analyze\n",
    "        games: Optional list of games to include (if None, will use all games in the framework)\n",
    "    \"\"\"\n",
    "    # If no games specified, get all games for the framework\n",
    "    if games is None:\n",
    "        games = framework_games[framework]\n",
    "    \n",
    "    # Create a color mapping for LLMs for consistency\n",
    "    llm_to_color = {llm: plt.cm.tab10(i % 10) for i, llm in enumerate(llms)}\n",
    "    \n",
    "    # Create a figure with subplots - one per game\n",
    "    n_games = len(games)\n",
    "    fig_height = n_games * height_per_game  # Dynamic height based on number of games\n",
    "    fig, axes = plt.subplots(n_games, 1, figsize=(12, fig_height), sharex=True, \n",
    "                           constrained_layout=False)\n",
    "    \n",
    "    # If there's only one game, make sure axes is still iterable\n",
    "    if n_games == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Keep track of the maximum score to set consistent x-axis limits\n",
    "    max_score = 0\n",
    "    \n",
    "    # Process each game\n",
    "    for game_idx, game in enumerate(games):\n",
    "        ax = axes[game_idx]\n",
    "        \n",
    "        # Collect scores and stds for this game across all LLMs\n",
    "        game_scores = []\n",
    "        game_stds = []\n",
    "        valid_llms = []\n",
    "        \n",
    "        for llm in llms:\n",
    "            try:\n",
    "                # Get all seeds for this game\n",
    "                seeds = llm_framework_scores[llm][framework][game]\n",
    "                \n",
    "                # Extract scores from all seeds\n",
    "                scores = [seed_data[0] for seed_data in seeds.values()]\n",
    "                \n",
    "                # Calculate mean and std\n",
    "                mean_score = np.mean(scores)\n",
    "                std_score = np.std(scores)\n",
    "                \n",
    "                game_scores.append(mean_score)\n",
    "                game_stds.append(std_score)\n",
    "                valid_llms.append(llm)\n",
    "                \n",
    "                # Update max score for x-axis scaling\n",
    "                if mean_score + std_score > max_score:\n",
    "                    max_score = mean_score + std_score\n",
    "                    \n",
    "            except (KeyError, ValueError):\n",
    "                # Skip if LLM doesn't have data for this game\n",
    "                continue\n",
    "        \n",
    "        # Sort scores, stds, and LLMs together (highest score first)\n",
    "        sorted_data = sorted(zip(game_scores, game_stds, valid_llms), key=lambda x: x[0], reverse=False)\n",
    "        \n",
    "        # Unpack the sorted data\n",
    "        if sorted_data:  # Check if there's any data\n",
    "            sorted_scores, sorted_stds, sorted_llms = zip(*sorted_data)\n",
    "        else:\n",
    "            # Skip this game if no data\n",
    "            ax.set_title(f\"{game} (No data)\")\n",
    "            continue\n",
    "            \n",
    "        # Set y positions for bars\n",
    "        y_positions = range(len(sorted_llms))\n",
    "        \n",
    "        # Draw horizontal bars\n",
    "        for i, (llm, score, std) in enumerate(zip(sorted_llms, sorted_scores, sorted_stds)):\n",
    "            # Draw the bar\n",
    "            ax.barh(\n",
    "                i,\n",
    "                score,\n",
    "                height=0.8,  # Reduced bar height to allow more space for text\n",
    "                label=llm if game_idx == 0 and i == 0 else \"\",  # Only add to legend once\n",
    "                color=llm_to_color[llm],\n",
    "                alpha=0.8\n",
    "            )\n",
    "            \n",
    "            # Add error bars\n",
    "            ax.errorbar(\n",
    "                score,\n",
    "                i,\n",
    "                xerr=std,\n",
    "                fmt='none',\n",
    "                ecolor='black',\n",
    "                capsize=3,\n",
    "                capthick=1,\n",
    "                elinewidth=1\n",
    "            )\n",
    "            \n",
    "            # Add score value at the end of each bar (now with percentage and split into two lines)\n",
    "            # Convert to percentage and format\n",
    "            score_pct = score * 100\n",
    "            std_pct = std * 100\n",
    "            \n",
    "            # Create a two-line label with percentage\n",
    "            label_score = f\"{score_pct:.1f}%\"\n",
    "            label_std = f\"{std_pct:.1f}%\"\n",
    "            \n",
    "            # Position the text with increased vertical offset for the two lines\n",
    "            # The key change is increasing the vertical offset (i - 0.25 vs i + 0.25)\n",
    "            spacing = .35\n",
    "            ax.text(\n",
    "                score + std + 0.01,  # Horizontal position\n",
    "                i - spacing,            # Increased vertical spacing (more negative)\n",
    "                label_std,\n",
    "                va='bottom',\n",
    "                fontsize=6           # Slightly reduced font size\n",
    "            )\n",
    "            ax.text(\n",
    "                score + std + 0.01,  # Same horizontal position\n",
    "                i + spacing,            # Increased vertical spacing (more positive)\n",
    "                label_score, \n",
    "                va='top',\n",
    "                fontsize=6           # Slightly reduced font size\n",
    "            )\n",
    "        \n",
    "        # Set y-ticks to be the LLM names\n",
    "        ax.set_yticks(y_positions)\n",
    "        shortened_llm_names = [llm.split('/')[-1] for llm in sorted_llms]  # Get just the model name without path\n",
    "        ax.set_yticklabels(shortened_llm_names)\n",
    "        \n",
    "        # Set title for this subplot\n",
    "        ax.set_title(game)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Only show x-label on the bottom plot\n",
    "        if game_idx == n_games - 1:\n",
    "            ax.set_xlabel('Normalized Score')\n",
    "    \n",
    "    # Set consistent x-axis limits and show as percentages\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0, max_score * 1.2)  # Add 20% padding\n",
    "        \n",
    "        # Format x-axis ticks as percentages\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0%}'))\n",
    "    \n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(\n",
    "        hspace=0.4,       # Increased space between subplots for better visualization\n",
    "        bottom=0.15,      # Bottom margin\n",
    "        left=0.15,        # Left margin for labels\n",
    "        right=0.85,       # Right margin\n",
    "        top=0.95          # Top margin\n",
    "    )\n",
    "    \n",
    "    # Save with bbox_inches to ensure nothing is cut off\n",
    "    plt.savefig(plot_name + '.png', bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.close()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "framework_save_title = {'textworld_express': 'textworld_express_all_games', 'textworld': 'textworld_all_games', 'alfworld': 'alfworld_all_games', 'scienceworld': 'scienceworld_all_games', 'jericho': 'jericho_all_games'}\n",
    "# Create the bar charts\n",
    "\n",
    "for framework, title in framework_save_title.items():\n",
    "    make_game_bar_charts(\n",
    "        llms=downsampled_llms, \n",
    "        framework=framework,\n",
    "        games=framework_games[framework],\n",
    "        height_per_game = 4.0,\n",
    "        plot_name=title\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_game_bar_charts(\n",
    "        llms=downsampled_llms, \n",
    "        framework=framework,\n",
    "        games=framework_games[framework],\n",
    "        height_per_game = 4.0,\n",
    "        plot_name=title\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Code: Experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top llm per game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = regen_and_filter_df()\n",
    "print(tmp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fws = tmp['framework'].unique()\n",
    "framework_best_llm = {}\n",
    "for fw in all_fws:\n",
    "    llm_scores = tmp[tmp['framework'] == fw]\n",
    "    framework_scores = llm_scores.groupby(\"llm\")['episode/normalized_highscore'].mean().reset_index()\n",
    "    top_score_row = framework_scores['episode/normalized_highscore'].idxmax()\n",
    "    top_llm = framework_scores.loc[top_score_row, 'llm']\n",
    "    framework_best_llm[fw] = top_llm    \n",
    "\n",
    "print(framework_best_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_optimal_logs(base_path = '/root/trajectories/'):\n",
    "    for framework, llm in framework_best_llm.items():\n",
    "        for data in filtered_runs_data:\n",
    "            if data['llm'] == llm and data['framework'] == framework:\n",
    "                convo_dict = {}\n",
    "                for i, step in enumerate(data['rollout']):\n",
    "                    turn = {}\n",
    "                    turn['user'] = step[5]\n",
    "                    turn['assistant'] = step[6]\n",
    "                    convo_dict[\"step \" + str(i)] = turn\n",
    "                # Save the conversation to a JSON file \n",
    "                with open(base_path + framework + \"_\" + data['game'] + \"_\" + llm.split(\"/\")[-1] + \"_\" + str(data['seed']) +  \".json\", 'w') as f:\n",
    "                    json.dump(convo_dict, f, indent=4)  \n",
    "                \n",
    "save_optimal_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
